{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train logistic regression with Keras\n",
    "Martin Kircher provided two training_data files: one is human readable and the other one is one-hot-encoded.\n",
    "Now we need to check that the information is actually in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. How many lines does the dataset have?\n",
    "Let's create a shuffled index list and store it in our system, to be able to create batches of data to pass to the fit_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40 s, sys: 8.62 s, total: 48.7 s\n",
      "Wall time: 48.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_imputed = \"/s/project/kipoi-cadd/data/raw/v1.3/training_data/training_data.imputed.csv\"\n",
    "shuffled_index_file = \"/s/project/kipoi-cadd/data/raw/v1.3/training_data/shuffle_splits/shuffled_index.pickle\"\n",
    "batches_index = \"/s/project/kipoi-cadd/data/raw/v1.3/training_data/shuffle_splits/batches_index.pickle\"\n",
    "training = \"/s/project/kipoi-cadd/data/raw/v1.3/training_data/training_data.tsv\"\n",
    "training_batches_folder = \"/s/project/kipoi-cadd/data/raw/v1.3/training_data/shuffle_splits/training/\"\n",
    "testing_batches_folder = \"/s/project/kipoi-cadd/data/raw/v1.3/training_data/shuffle_splits/testing/\"\n",
    "validation_batches_folder = \"/s/project/kipoi-cadd/data/raw/v1.3/training_data/shuffle_splits/validation/\"\n",
    "with open(shuffled_index_file, 'rb') as f:\n",
    "    shuffled_index = pickle.load(f)\n",
    "with open(batches_index, 'rb') as f:\n",
    "    batches_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select idex list for batch and extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(directory, batch_size, sep=','):\n",
    "    # Add for loop because the generator has to be able to start itself again\n",
    "    sub_batch = None\n",
    "    for file in os.listdir(directory):\n",
    "        filename = directory + str(os.fsdecode(file))\n",
    "        rows_df = pd.read_csv(filename, sep=sep)\n",
    "        rows_df.y = [0 if r == -1 else r for r in rows_df.y]\n",
    "        sub = (rows_df.shape[0] // batch_size) + 1\n",
    "        for i in range(sub):\n",
    "            start = (i) * batch_size\n",
    "            if sub_batch is None:\n",
    "                end = min(rows_df.shape[0], start + batch_size)\n",
    "                sub_batch = rows_df.iloc[start:end,:]\n",
    "            else:\n",
    "                end = batch_size - sub_batch.shape[0]\n",
    "                sub_batch = sub_batch.append(rows_df.iloc[start:end,:])\n",
    "            if sub_batch.shape[0] == batch_size:\n",
    "                yield (sub_batch.iloc[:,1:], sub_batch.iloc[:,0])\n",
    "                sub_batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/s/project/kipoi-cadd/data/raw/v1.3/training_data/shuffle_splits/training/1.pickle\n",
      "(3200, 1063) (3200,)\n",
      "(3200, 1063) (3200,)\n",
      "(3200, 1063) (3200,)\n",
      "/s/project/kipoi-cadd/data/raw/v1.3/training_data/shuffle_splits/training/2.pickle\n",
      "(3200, 1063) (3200,)\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "this_value = None\n",
    "for batch in load_batch(training_batches_folder, 3200):\n",
    "    this_value = batch\n",
    "    print(this_value[0].shape, this_value[1].shape)\n",
    "    if i ==4: break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 1063)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_value[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use single threaded generator and feed it to keras.models.fit_generator()\n",
    "Previous tests have \"demonstrated\" that multithreading only makes things slower. Now, we use a generator that yields a single line, as required by the `fit_generator` method from `keras.models`. Right now, I'm taking inspiration from this tutorial in Medium: [Simple Logistic Regression using Keras](https://medium.com/@the1ju/simple-logistic-regression-using-keras-249e0cc9a970)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 1485s 15ms/step - loss: 7.9650 - acc: 0.5004\n",
      "Test score: 7.979681264748594\n",
      "Test accuracy: 0.4994675\n",
      "CPU times: user 37min 50s, sys: 2min 55s, total: 40min 46s\n",
      "Wall time: 36min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build the model\n",
    "output_dim = 1 # One binary class\n",
    "input_dim = 1063 # number of features of the input (102 for training, and 1063 for training_imputed)\n",
    "model = Sequential() \n",
    "model.add(Dense(output_dim, input_dim=input_dim, activation='softmax'))\n",
    "batch_size = 64\n",
    "nb_epoch = 1\n",
    "nb_steps_training = 100000 # 34693009 / batch_size = 542078.265625\n",
    "nb_steps_prediction = 50000 # 350051 / batch_size = 5469.546875\n",
    "\n",
    "training_generator = load_batch(training_batches_folder, batch_size)\n",
    "testing_generator = load_batch(testing_batches_folder, batch_size)\n",
    "validation_generator = load_batch(validation_batches_folder, batch_size)\n",
    "\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring one epoch finished\n",
    "and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None\n",
    "is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined.\n",
    "\"\"\"\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy']) \n",
    "# history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,verbose=1, validation_data=(X_test, Y_test)) \n",
    "history = model.fit_generator(training_generator, steps_per_epoch=nb_steps_training, epochs=nb_epoch, shuffle=False, verbose=1)\n",
    "score = model.evaluate_generator(testing_generator, steps=nb_steps_prediction, max_queue_size=10)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predict on a validation set\n",
    "See if the results are close to CADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_generator(validation_generator, steps=nb_steps_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = (\"/s/project/kipoi-cadd/data/models/model\" +\n",
    "                  datetime.datetime.now().strftime(\"%Y.%m.%d_%H:%M:%S\") +\n",
    "                  \".pickle\")\n",
    "with open(model_file, 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check if batch splitting worked\n",
    "Lines match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines = 3060\n",
      "35043060\n"
     ]
    }
   ],
   "source": [
    "# Check indexes\n",
    "len(batches_index)\n",
    "sum_lines = 0\n",
    "for b in batches_index:\n",
    "    lines = len(b.get('index_list'))\n",
    "    if lines != 10000:\n",
    "        print(\"lines =\", lines)\n",
    "    sum_lines += lines\n",
    "print(sum_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check batches\n",
    "total_num_examples = 35043061 - 1\n",
    "current = 35046565 - 3505\n",
    "diff = total_num_examples - current\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.008995789324587"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the sizes of train and test\n",
    "#! zcat train.csv.gz | wc -l\n",
    "train_size = 34693009\n",
    "#! zcat test.csv.gz | wc -l\n",
    "test_size = 350051\n",
    "(test_size / train_size)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6619, 1064)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check testing batches\n",
    "output = (\"/s/project/kipoi-cadd/data/raw/v1.3/training_data/\" +\n",
    "              \"shuffle_splits/tests/\")\n",
    "batch = pd.read_csv(output+'3.csv')\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split last batch to get exact amount\n",
    "path = (\"/s/project/kipoi-cadd/data/raw/v1.3/training_data/\" +\n",
    "              \"shuffle_splits/\")\n",
    "split_batch = pd.read_csv(path + \"tests/3470.csv\", index_col=0)\n",
    "train = split_batch.iloc[:3009,:]\n",
    "test = split_batch.iloc[3009:,:]\n",
    "train.to_csv(path + \"training/3469.csv\", mode='a')\n",
    "test.to_csv(path + \"tests/3470.csv\")\n",
    "\"\"\"\n",
    "Testing = 350087-36=350051\n",
    "Training = 34696478-3469=34693009\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kipoi-cadd2]",
   "language": "python",
   "name": "conda-env-kipoi-cadd2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
