{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate LMDB by batches\n",
    "First we will test the functionality in place and check it works for a small amount of batches before generating the whole DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kipoi_cadd.data import CaddBatchDataset, cadd_serialize_numpy_row\n",
    "from kipoi_cadd.data_utils import dir_batch_generator, OrderedDict, get_one_batch\n",
    "from kipoi_cadd.utils import dump_to_pickle, load_pickle\n",
    "import time\n",
    "import pyarrow as pa\n",
    "import lmdb\n",
    "from tqdm import tqdm\n",
    "import blosc\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = \"/s/project/kipoi-cadd/data/raw/v1.3/training_data/\"\n",
    "lmdb_dir = training_dir + \"lmdb\"\n",
    "lmdb_batch_dir = training_dir + \"lmdb_batched\"\n",
    "csv_file = training_dir + \"training_data.imputed.csv\"\n",
    "valid_id_file = training_dir + \"valid_idx.pkl\"\n",
    "all_ids_file = training_dir + \"variant_ids.pkl\"\n",
    "ids_10k_file = training_dir + \"sample_indices/ids_10k.pkl\"\n",
    "std_scaler_file = training_dir + \"stats/standard_scaler_first10k.pkl\"\n",
    "short_csv = training_dir + \"last10k.csv\"\n",
    "batch_idx_file = training_dir + \"shuffle_splits/batch_idxs_256.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2147483647"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.iinfo(np.int32).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lmdb_from_iterator(it, lmdb_batch_dir, variant_ids_file, num_batches=-1,\n",
    "                              map_size=23399354270):\n",
    "    start = time.time()\n",
    "\n",
    "    index_mapping = OrderedDict()\n",
    "    map_size = None\n",
    "    txn = None\n",
    "    batch_num = 0\n",
    "    variant_ids = load_pickle(variant_ids_file)\n",
    "\n",
    "    env = lmdb.Environment(lmdb_batch_dir, map_size=map_size, max_dbs=0, lock=False)\n",
    "    with env.begin(write=True, buffers=True) as txn:\n",
    "        for batch in tqdm(it, total=num_batches):\n",
    "            b = {\n",
    "                \"batch_id\": np.int32(batch_num),\n",
    "                \"inputs\": batch[0].values.astype(np.float16),\n",
    "                \"targets\": batch[1].values.astype(np.float16),\n",
    "                \"metadata\": {\n",
    "                    \"row_num\": np.array(batch[0].index, dtype=np.int32),\n",
    "                    \"variant_id\": np.array(variant_ids.loc[batch[0].index], dtype='<U20')\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Serialize and compress\n",
    "            buff = pa.serialize(b).to_buffer()\n",
    "            blzpacked = blosc.compress(buff, typesize=8, cname='blosclz')\n",
    "\n",
    "            try:\n",
    "                txn.put(str(batch_num).encode('ascii'), blzpacked)\n",
    "            except lmdb.MapFullError as err:\n",
    "                print(str(err) + \". Exiting the program.\")\n",
    "\n",
    "            batch_num += 1\n",
    "            if batch_num >= num_batches: break\n",
    "\n",
    "    print(\"Finished putting \" + str(batch_num) + \" batches to lmdb.\")\n",
    "    end = time.time()\n",
    "    print(\"Total elapsed time: {:.2f} minutes.\".format(\n",
    "        (end - start) / 60))\n",
    "\n",
    "def calculate_map_size(row_example, nrows, multiplier=1.9):\n",
    "    row_size = pa.serialize(row_example).to_buffer().size\n",
    "    map_size = int(row_size * nrows * multiplier)\n",
    "    return map_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = dir_batch_generator(training_dir + \"shuffle_splits/training/\", 256)\n",
    "test_batch = next(it)\n",
    "variant_ids = load_pickle(all_ids_file)\n",
    "nrows = len(variant_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_example = {\n",
    "    \"batch_id\": np.int32(0),\n",
    "    \"inputs\": test_batch[0].values.astype(np.float16),\n",
    "    \"targets\": test_batch[1].values.astype(np.float16),\n",
    "    \"metadata\": {\n",
    "        \"row_num\": np.array(test_batch[0].index, dtype=np.int32),\n",
    "        \"variant_id\": np.array(variant_ids.loc[test_batch[0].index], dtype='<U20')\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:07<00:14,  7.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished putting 3 batches to lmdb.\n",
      "Total elapsed time: 0.29 minutes.\n"
     ]
    }
   ],
   "source": [
    "ms = calculate_map_size(row_example, 3)\n",
    "it = dir_batch_generator(training_dir + \"shuffle_splits/training/\", 256)\n",
    "create_lmdb_from_iterator(it, lmdb_batch_dir, all_ids_file, num_batches=3, map_size=ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_one_batch(lmdb_batch_dir, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1063)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['inputs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aja = load_pickle(batch_idx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(batch['metadata']['variant_id'] == np.array(list(aja[2][\"variant_ids\"]), dtype='<U20'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'row_nrs': <kipoi_cadd.utils.OrderedSet at 0x7f3d64ed5160>,\n",
       " 'variant_ids': <kipoi_cadd.utils.OrderedSet at 0x7f3d64ed54e0>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aja[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CaddBatchDataset(lmdb_dir, batch_idx_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kipoi-cadd2]",
   "language": "python",
   "name": "conda-env-kipoi-cadd2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
