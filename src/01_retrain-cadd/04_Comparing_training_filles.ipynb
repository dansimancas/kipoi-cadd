{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing training files\n",
    "Martin Kircher provided two training_data files: one is human readable and the other one is one-hot-encoded.\n",
    "Now we need to check that the information is actually in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.model_selection as sklearn_sel\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in dataset with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>Chrom</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Alt</th>\n",
       "      <th>Type</th>\n",
       "      <th>Length</th>\n",
       "      <th>isTv</th>\n",
       "      <th>Consequence</th>\n",
       "      <th>GC</th>\n",
       "      <th>...</th>\n",
       "      <th>SIFTcat</th>\n",
       "      <th>SIFTval</th>\n",
       "      <th>mirSVR-Score.na</th>\n",
       "      <th>targetScan.na</th>\n",
       "      <th>cDNApos.na</th>\n",
       "      <th>CDSpos.na</th>\n",
       "      <th>protPos.na</th>\n",
       "      <th>Grantham.na</th>\n",
       "      <th>PolyPhenVal.na</th>\n",
       "      <th>SIFTval.na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>379177</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>SNV</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.48</td>\n",
       "      <td>...</td>\n",
       "      <td>UD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>379274</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>SNV</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.54</td>\n",
       "      <td>...</td>\n",
       "      <td>UD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>379476</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>SNV</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UP</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>UD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>379631</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>SNV</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UP</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>UD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>379724</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>SNV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>UP</td>\n",
       "      <td>0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>UD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   y  Chrom     Pos Ref Alt Type  Length  isTv Consequence    GC     ...      \\\n",
       "0  0      1  379177   T   G  SNV       0     1          NC  0.48     ...       \n",
       "1  0      1  379274   C   G  SNV       0     1          NC  0.54     ...       \n",
       "2  0      1  379476   A   T  SNV       0     1          UP  0.63     ...       \n",
       "3  0      1  379631   C   G  SNV       0     1          UP  0.34     ...       \n",
       "4  0      1  379724   A   G  SNV       0     0          UP  0.28     ...       \n",
       "\n",
       "   SIFTcat  SIFTval  mirSVR-Score.na  targetScan.na  cDNApos.na  CDSpos.na  \\\n",
       "0       UD      0.0                1              1           0          1   \n",
       "1       UD      0.0                1              1           0          1   \n",
       "2       UD      0.0                1              1           1          1   \n",
       "3       UD      0.0                1              1           1          1   \n",
       "4       UD      0.0                1              1           1          1   \n",
       "\n",
       "   protPos.na  Grantham.na  PolyPhenVal.na  SIFTval.na  \n",
       "0           1            1               1           1  \n",
       "1           1            1               1           1  \n",
       "2           1            1               1           1  \n",
       "3           1            1               1           1  \n",
       "4           1            1               1           1  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = \"/s/project/kipoi-cadd/data/raw/v1.3/training_data/training_data.tsv\"\n",
    "training_imputed = \"/s/project/kipoi-cadd/data/raw/v1.3/training_data/training_data.imputed.csv\"\n",
    "\n",
    "training_dd = dd.read_csv(training, sep='\\t', \n",
    "                          dtype={'PolyPhenVal': 'float64',\n",
    "                                 'SIFTval': 'float64',\n",
    "                                 'mirSVR-E': 'float64',\n",
    "                                 'mirSVR-Score': 'float64',\n",
    "                                 'relCDSpos': 'float64',\n",
    "                                 'relProtPos': 'float64'})\n",
    "\n",
    "training_imp_dd = dd.read_csv(training_imputed, assume_missing=True)\n",
    "training_dd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a. Process with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ny = training_imp_dd.y\\nX = training_imp_dd.drop('y', axis=1)\\nX_small = X.loc[:1000,:]\\ny_small = y.loc[:1000]\\n\\nX_small = X_small.compute(n_workers)\\ny_small = y_small.compute()]\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "y = training_imp_dd.y\n",
    "X = training_imp_dd.drop('y', axis=1)\n",
    "X_small = X.loc[:1000,:]\n",
    "y_small = y.loc[:1000]\n",
    "\n",
    "X_small = X_small.compute(n_workers)\n",
    "y_small = y_small.compute()]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b. Read in some rows of the dataframe with pandas and then convert to dask\n",
    "Since I'm not so fluent with Dask I need the trial-and-error approach. For this purpose, I will select a subset of the dataframe using pandas and then read it as a Dask dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pandas = pd.read_csv(training_imputed, nrows=10000)\n",
    "y_pandas = pd.read_csv(training, sep='\\t', nrows=10000)[[\"y\"]]\n",
    "y_pandas = y_pandas.y\n",
    "\n",
    "# Inserting some artificial positive examples\n",
    "np.random.seed(10)\n",
    "msk = np.random.rand(len(y_pandas)) < 0.2\n",
    "y_pandas[msk] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn_sel.train_test_split(X_pandas, y_pandas, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droped 824 constant cols.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/modules/i12g/anaconda/3-5.0.1/envs/kipoi-cadd2/lib/python3.6/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# Drop constant columns\n",
    "constant_cols = set()\n",
    "for dataset in [X_pandas]:\n",
    "    for col in dataset:\n",
    "        # print(dataset.shape, type(dataset))\n",
    "        # print(len(np.unique(dataset[col])))\n",
    "        if len(np.unique(dataset[col])) < 20 and col != 'y':\n",
    "            constant_cols.add(col)\n",
    "print(\"Droped\", len(constant_cols), \"constant cols.\")\n",
    "\n",
    "for dataset in [X_train, X_test, y_train, y_test]:\n",
    "    dataset.drop(columns=list(constant_cols), inplace=True, errors='ignore')\n",
    "\n",
    "X_pandas.drop(columns=list(constant_cols), inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dd.from_pandas(X_pandas, chunksize=100)\n",
    "y = dd.from_pandas(y_pandas, chunksize=100)\n",
    "X = X.drop('y', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.dataframe.core.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1.0, max_iter=10, multiclass='ovr', n_jobs=64,\n",
       "          penalty='l2', random_state=None, solver='lbfgs',\n",
       "          solver_kwargs=None, tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', solver='lbfgs', n_jobs=64, max_iter=10)\n",
    "lr.fit(X_train.values, y_train.values) # If leave just the dataframe, will throw an error saying \"This estimator does not support dask dataframes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = lr.predict(X_train.values)\n",
    "lr.predict_proba(X_test.values)\n",
    "scores_train = lr.score(X_train.values, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read like \n",
      "[[TN, FP], \n",
      "[FN, TP]]\n",
      "\n",
      "[[6221   16]\n",
      " [1680   29]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7946"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def confusion_matrix_dask(truth, predictions, labels_list=[]):\n",
    "    TP=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    TN=0\n",
    "    if not labels_list:\n",
    "        TP=(truth[predictions==1]==1).sum()\n",
    "        FN=(truth[predictions!=1]==1).sum()\n",
    "        TN=(truth[predictions!=1]!=1).sum()\n",
    "        FP=(truth[predictions==1]!=1).sum()\n",
    "    for label in labels_list:\n",
    "        TP=(truth[predictions==label]==label).sum()+TP\n",
    "        FN=(truth[predictions!=label]==label).sum()+FP\n",
    "        TN=(truth[predictions!=label]!=label).sum()+TN\n",
    "        FP=(truth[predictions==label]!=label).sum()+FN\n",
    "\n",
    "    TN, FP, FN, TP = dask.compute(TN, FP, FN, TP)\n",
    "    return(TN, FP, FN, TP)\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix_dask(y_train.values, y_train_pred)\n",
    "print(\"Read like \\n[[TN, FP], \\n[FN, TP]]\\n\")\n",
    "print(np.array([[TN, FP], [FN ,TP]]))\n",
    "sum([TP, FP, TN, FN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "y_train_pred_computed = y_train_pred.compute()\n",
    "scores_train_computed = scores_train.compute()\n",
    "\"\"\"\n",
    "# saving model as pickle\n",
    "import pickle\n",
    "save_model = \"/s/project/kipoi-cadd/data/processed/kipoi_cadd_models/lr.pickle\"\n",
    "pickle.dump(lr, open(save_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read like \n",
      "[[TN, FP], \n",
      "[FN, TP]]\n",
      "\n",
      "[[1638   13]\n",
      " [ 400    3]]\n",
      "2054\n",
      "2054\n"
     ]
    }
   ],
   "source": [
    "my_model = pickle.load(open(save_model, \"rb\"))\n",
    "y_test_pred = my_model.predict(X_test.values)\n",
    "TN_test, FP_test, FN_test, TP_test = confusion_matrix_dask(y_test.values, y_test_pred)\n",
    "print(\"Read like \\n[[TN, FP], \\n[FN, TP]]\\n\")\n",
    "print(np.array([[TN_test, FP_test], [FN_test ,TP_test]]))\n",
    "print(sum([TP_test, FP_test, TN_test, FN_test]))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read like \n",
      " tn, fp, \n",
      " fn, tp \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6221,   16],\n",
       "       [1680,   29]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_train_computed = y_train.compute()\n",
    "y_train_pred_computed = y_train_pred.compute()\n",
    "print(\"Read like \\n tn, fp, \\n fn, tp \\n\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_train_computed, y_train_pred_computed).ravel()\n",
    "confusion_matrix(y_train_computed, y_train_pred_computed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kipoi-cadd2]",
   "language": "python",
   "name": "conda-env-kipoi-cadd2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
