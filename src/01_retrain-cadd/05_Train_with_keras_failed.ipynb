{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train logistic regression with Keras\n",
    "Martin Kircher provided two training_data files: one is human readable and the other one is one-hot-encoded.\n",
    "Now we need to check that the information is actually in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sklearn\n",
    "import csv\n",
    "import sys\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from Crypto.Random.random import randint\n",
    "import dask.dataframe as dd\n",
    "from itertools import islice\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. How many lines does the dataset have?\n",
    "Let's create a shuffled index list and store it in our system, to be able to create batches of data to pass to the fit_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_imputed = \"/s/project/kipoi-cadd/data/raw/v1.3/training_data/training_data.imputed.csv\"\n",
    "shuffled_index_file = \"/s/project/kipoi-cadd/data/raw/v1.3/training_data/shuffle_splits/shuffled_index.pickle\"\n",
    "training = \"/s/project/kipoi-cadd/data/raw/v1.3/training_data/training_data.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "size_training_set = !cat {training_imputed}  | wc -l # 35043061\n",
    "size_training_set = int(size_training_set[0]) - 1 # we remove the header line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def onthefly(n):\n",
    "    numbers=np.arange(n,dtype=np.uint32)\n",
    "    for i in range(n):\n",
    "        j = randint(i, n-1)\n",
    "        numbers[i], numbers[j] = numbers[j], numbers[i]\n",
    "        yield numbers[i]\n",
    "\n",
    "gen = onthefly(size_training_set)\n",
    "shuffled_index = []\n",
    "for i in range(size_training_set):\n",
    "    shuffled_index.append(next(gen))\n",
    "shuffled_index[:20]\n",
    "\n",
    "with open(training_imputed, 'wb') as f:\n",
    "    pickle.dump(shuffled_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(shuffled_index_file, 'rb') as f:\n",
    "    shuffled_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select idex list for batch and extract with dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_df(df, list_index):\n",
    "    print(df.shape)\n",
    "    print(df.index.values[:5])\n",
    "    subset = set(list_index) & set(df.index)\n",
    "    return(df.loc[subset])\n",
    "\n",
    "\n",
    "def get_batch_indexes(index_list, batch_size, file, output, num_batches=None, sep=','):\n",
    "    num_indexes = len(index_list)\n",
    "    amount_batches = (num_indexes // batch_size) + 1\n",
    "    num_loops = min(num_batches, amount_batches) if num_batches else amount_batches\n",
    "    batch_indexes = []\n",
    "    \n",
    "    print(amount_batches, num_loops)\n",
    "    \n",
    "    for i in range(num_loops):\n",
    "        start = (i)*batch_size\n",
    "        end = min(num_indexes, start + batch_size)\n",
    "        extraction = {\n",
    "            'file': file,\n",
    "            'output': output+str(i+1)+\".pickle\",\n",
    "            'sep': sep,\n",
    "            'index_list': set(index_list[start:end])\n",
    "        }\n",
    "        batch_indexes.append(extraction)\n",
    "    \n",
    "    return(batch_indexes)\n",
    "        \n",
    "\n",
    "def generate_batch(file, index_list, batch_size, num_batches, sep=','):\n",
    "    with open(file) as input_file:\n",
    "        reader = csv.reader(input_file, delimiter=sep)\n",
    "        header = next(reader)\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            start = (batch)*batch_size\n",
    "            end = min(len(index_list), start + batch_size)\n",
    "            batch_indexes = set(index_list[start:end])\n",
    "\n",
    "            desired_rows = [row for row_number, row in enumerate(reader)\n",
    "                            if row_number in batch_indexes]\n",
    "            \n",
    "            rows_df = pd.DataFrame(desired_rows, index=batch_indexes, columns=header)\n",
    "            rows_df.y = [0 if r == -1 else r for r in rows_df.y]\n",
    "            yield (rows_df.iloc[:,1:], rows_df.y)\n",
    "\n",
    "            if end == len(index_list):\n",
    "                break\n",
    "\n",
    "\n",
    "def generate_one_batch(file, index_list, sep=','):\n",
    "    with open(file) as input_file:\n",
    "        reader = csv.reader(input_file, delimiter=sep)\n",
    "        header = next(reader)\n",
    "\n",
    "        desired_rows = [row for row_number, row in enumerate(reader)\n",
    "                        if row_number in index_list]\n",
    "\n",
    "        yield pd.DataFrame(desired_rows, index=index_list, columns=header)\n",
    "\n",
    "\n",
    "def yield_one_line(file, index_list, batch_size, sep=','):\n",
    "    with open(file) as input_file:\n",
    "        reader = csv.reader(input_file, delimiter=sep)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            ix = index_list[i]\n",
    "            row = next(islice(reader, int(ix), None))\n",
    "            # yield row\n",
    "            row[0] = 0 if row[0] == -1 else 1\n",
    "            row_df = pd.DataFrame([row], index=[ix], columns=header)\n",
    "            yield (row_df.iloc[:,1:], row_df.y)\n",
    "            reader = next(next(csv.reader(input_file, delimiter=sep)))\n",
    "\n",
    "\n",
    "def generate_one_batch_noattributes(extraction):\n",
    "    file = extraction.get('file')\n",
    "    output = extraction.get('output')\n",
    "    sep = extraction.get('sep')\n",
    "    index_list = extraction.get('index_list')\n",
    "\n",
    "    with open(file) as input_file:\n",
    "        reader = csv.reader(input_file, delimiter=sep)\n",
    "        header = next(reader)\n",
    "\n",
    "        desired_rows = [row for row_number, row in enumerate(reader)\n",
    "                        if row_number in index_list]\n",
    "\n",
    "        df = pd.DataFrame(desired_rows, index=index_list, columns=header)\n",
    "        with open(output, 'wb') as f:\n",
    "            pickle.dump(df, f)\n",
    "\n",
    "\n",
    "def generate_one_batch_singlethreaded(extraction):\n",
    "    for ex in extraction:\n",
    "        file = ex.get('file')\n",
    "        output = ex.get('output')\n",
    "        sep = ex.get('sep')\n",
    "        index_list = ex.get('index_list')\n",
    "\n",
    "        with open(file) as input_file:\n",
    "            reader = csv.reader(input_file, delimiter=sep)\n",
    "            header = next(reader)\n",
    "\n",
    "            desired_rows = [row for row_number, row in enumerate(reader)\n",
    "                            if row_number in index_list]\n",
    "\n",
    "            df = pd.DataFrame(desired_rows, index=index_list, columns=header)\n",
    "            with open(output, 'wb') as f:\n",
    "                pickle.dump(df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThreads = 1, num_batches = 2\\nCPU times: user 8min 14s, sys: 32.2 s, total: 8min 46s\\nWall time: 8min 46s\\n\\nThreads = 2, num_batches = 2\\nCPU times: user 9min 21s, sys: 46.3 s, total: 10min 7s\\nWall time: 9min 41s\\n\\nThreads = 5, num_batches = 2\\nCPU times: user 9min 21s, sys: 44.1 s, total: 10min 5s\\nWall time: 9min 41s\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Threads = 1, num_batches = 2\n",
    "CPU times: user 8min 14s, sys: 32.2 s, total: 8min 46s\n",
    "Wall time: 8min 46s\n",
    "\n",
    "Threads = 2, num_batches = 2\n",
    "CPU times: user 9min 21s, sys: 46.3 s, total: 10min 7s\n",
    "Wall time: 9min 41s\n",
    "\n",
    "Threads = 5, num_batches = 2\n",
    "CPU times: user 9min 21s, sys: 44.1 s, total: 10min 5s\n",
    "Wall time: 9min 41s\n",
    "\n",
    "Threads = 3, num_batches = 3\n",
    "CPU times: user 16min 54s, sys: 1min 46s, total: 18min 41s\n",
    "Wall time: 16min 38s\n",
    "\n",
    "Threads = 1, num_batches = 3\n",
    "CPU times: user 12min 21s, sys: 47.8 s, total: 13min 9s\n",
    "Wall time: 13min 9s\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 54s, sys: 1min 46s, total: 18min 41s\n",
      "Wall time: 16min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test paralellism\n",
    "pool = ThreadPool(3)\n",
    "results = pool.map(generate_one_batch_noattributes, batch_indexes) # 9min 35s\n",
    "pool.close() \n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 21s, sys: 47.8 s, total: 13min 9s\n",
      "Wall time: 13min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate_one_batch_singlethreaded(batch_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use single threaded generator and feed it to keras.models.fit_generator()\n",
    "Previous tests have \"demonstrated\" that multithreading only makes things slower. Now, we use a generator that yields a single line, as required by the `fit_generator` method from `keras.models`. Right now, I'm taking inspiration from this tutorial in Medium: [Simple Logistic Regression using Keras](https://medium.com/@the1ju/simple-logistic-regression-using-keras-249e0cc9a970)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 0 ns, total: 8 µs\n",
      "Wall time: 15.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# batch_generator = yield_one_line(training, shuffled_index, sep='\\t')\n",
    "batch_generator = generate_batch(training_imputed, shuffled_index, 32, 1, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1063)\n",
      "CPU times: user 28min 30s, sys: 50.1 s, total: 29min 21s\n",
      "Wall time: 29min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample = next(batch_generator) # CPU times: user 28min 30s, sys: 50.1 s, total: 29min 21s. Wall time: 29min 19s\n",
    "print(sample[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1722s 1722s/step - loss: 15.9424 - acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 7s 7s/step - loss: 15.9424 - acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 15.9424 - acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 15.9424 - acc: 0.5000\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/modules/i12g/anaconda/3-5.0.1/envs/kipoi-cadd2/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/modules/i12g/anaconda/3-5.0.1/envs/kipoi-cadd2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/modules/i12g/anaconda/3-5.0.1/envs/kipoi-cadd2/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build the model\n",
    "output_dim = 1 # One binary class\n",
    "input_dim = 1063 # number of features of the input (102 for training, and 1063 for training_imputed)\n",
    "model = Sequential() \n",
    "model.add(Dense(output_dim, input_dim=input_dim, activation='softmax'))\n",
    "batch_size = 32\n",
    "nb_epoch = 10\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy']) \n",
    "# history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,verbose=1, validation_data=(X_test, Y_test)) \n",
    "history = model.fit_generator(batch_generator, steps_per_epoch=1, epochs=nb_epoch, workers=4, use_multiprocessing=True, shuffle=False)\n",
    "score = model.evaluate(X_test, Y_test, verbose=0) \n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in csv by batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = \"/s/project/kipoi-cadd/data/raw/v1.3/training_data/training_data.tsv\"\n",
    "training_imputed = \"/s/project/kipoi-cadd/data/raw/v1.3/training_data/training_data.imputed.csv\"\n",
    "\n",
    "def get_first_n_batches(file, num_batches, delimiter=','):\n",
    "    with open(file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=delimiter)\n",
    "        header = next(reader) # skip header\n",
    "\n",
    "        batch_size = 10000\n",
    "        batch = []\n",
    "        count = 0\n",
    "        count_batches = 0\n",
    "\n",
    "        for row in reader:\n",
    "            if count >= batch_size:\n",
    "                yield pd.DataFrame(batch, columns=header)\n",
    "                count_batches += 1\n",
    "                batch = []\n",
    "                count = 0\n",
    "\n",
    "            batch.append(row)\n",
    "            count += 1\n",
    "            \n",
    "            if count_batches >= num_batches:\n",
    "                break\n",
    "\n",
    "\n",
    "def get_batch_from_line_isslice(file, start_line, delimiter=','):\n",
    "    with open(file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=delimiter)\n",
    "        header = next(reader) # skip header\n",
    "\n",
    "        batch_size = 1000000\n",
    "        batch = []\n",
    "        count = 0\n",
    "        line = start_line\n",
    "        index = []\n",
    "        \n",
    "\n",
    "        for row in islice(reader, start_line, None):\n",
    "            if count >= batch_size:\n",
    "                yield pd.DataFrame(batch, columns=header, index=index)\n",
    "                break\n",
    "\n",
    "            batch.append(row)\n",
    "            index.append(line)\n",
    "            count += 1\n",
    "            line += 1\n",
    "\n",
    "def get_batch_from_line_skip(file, start_line, delimiter=','):\n",
    "    with open(file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=delimiter)\n",
    "        header = next(reader) # skip header\n",
    "\n",
    "        batch_size = 1000000\n",
    "        batch = []\n",
    "        count = 0\n",
    "        line = 0\n",
    "\n",
    "        for _ in range(start_line): # skip the first start_line rows\n",
    "            next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            if count >= batch_size:\n",
    "                yield pd.DataFrame(batch, columns=header)\n",
    "                break\n",
    "\n",
    "            batch.append(row)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positive_class(file, num_pos, delimiter=','):\n",
    "    with open(file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=delimiter)\n",
    "        header = next(reader) # skip header\n",
    "        positive_examples = []\n",
    "        count_pos = 0\n",
    "        index = []\n",
    "        line_num = -1\n",
    "        \n",
    "        for row in reader:\n",
    "            line_num += 1\n",
    "            if count_pos >= num_pos:\n",
    "                yield pd.DataFrame(positive_examples, columns=header, index=index)\n",
    "                # yield row[0]\n",
    "                break\n",
    "            if row[0] != '0':\n",
    "                positive_examples.append(row)\n",
    "                index.append(line_num)\n",
    "                count_pos += 1\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 39s, sys: 7.26 s, total: 2min 46s\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pos_examples = next(find_positive_class(training, 10, delimiter='\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>Chrom</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Alt</th>\n",
       "      <th>Type</th>\n",
       "      <th>Length</th>\n",
       "      <th>isTv</th>\n",
       "      <th>Consequence</th>\n",
       "      <th>GC</th>\n",
       "      <th>...</th>\n",
       "      <th>SIFTcat</th>\n",
       "      <th>SIFTval</th>\n",
       "      <th>mirSVR-Score.na</th>\n",
       "      <th>targetScan.na</th>\n",
       "      <th>cDNApos.na</th>\n",
       "      <th>CDSpos.na</th>\n",
       "      <th>protPos.na</th>\n",
       "      <th>Grantham.na</th>\n",
       "      <th>PolyPhenVal.na</th>\n",
       "      <th>SIFTval.na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17521530</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>887521</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>SNV</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>CS</td>\n",
       "      <td>0.58</td>\n",
       "      <td>...</td>\n",
       "      <td>UD</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17521531</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>887553</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>SNV</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td>0.55</td>\n",
       "      <td>...</td>\n",
       "      <td>UD</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17521532</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>887952</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>SNV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NS</td>\n",
       "      <td>0.59</td>\n",
       "      <td>...</td>\n",
       "      <td>deleterious</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17521533</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>887985</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>SNV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>0.60</td>\n",
       "      <td>...</td>\n",
       "      <td>UD</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17521534</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>887987</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>SNV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>0.60</td>\n",
       "      <td>...</td>\n",
       "      <td>UD</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17521535</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>888184</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>SNV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>0.68</td>\n",
       "      <td>...</td>\n",
       "      <td>UD</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17521536</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>888213</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>SNV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>0.67</td>\n",
       "      <td>...</td>\n",
       "      <td>UD</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17521537</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>888503</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>SNV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>UD</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17521538</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>888504</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>SNV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>UD</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17521539</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>888537</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>SNV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>UD</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          y Chrom     Pos Ref Alt Type Length isTv Consequence    GC  \\\n",
       "17521530  1     1  887521   T   G  SNV      0    1          CS  0.58   \n",
       "17521531  1     1  887553   G   T  SNV      0    1           I  0.55   \n",
       "17521532  1     1  887952   C   T  SNV      0    0          NS  0.59   \n",
       "17521533  1     1  887985   G   A  SNV      0    0           S  0.60   \n",
       "17521534  1     1  887987   A   G  SNV      0    0           S  0.60   \n",
       "17521535  1     1  888184   T   C  SNV      0    0           I  0.68   \n",
       "17521536  1     1  888213   C   T  SNV      0    0           I  0.67   \n",
       "17521537  1     1  888503   C   T  SNV      0    0           I  0.62   \n",
       "17521538  1     1  888504   G   A  SNV      0    0           I  0.62   \n",
       "17521539  1     1  888537   C   T  SNV      0    0           I  0.62   \n",
       "\n",
       "            ...          SIFTcat SIFTval mirSVR-Score.na targetScan.na  \\\n",
       "17521530    ...               UD       0               1             1   \n",
       "17521531    ...               UD       0               1             1   \n",
       "17521532    ...      deleterious       0               1             1   \n",
       "17521533    ...               UD       0               1             1   \n",
       "17521534    ...               UD       0               1             1   \n",
       "17521535    ...               UD       0               1             1   \n",
       "17521536    ...               UD       0               1             1   \n",
       "17521537    ...               UD       0               1             1   \n",
       "17521538    ...               UD       0               1             1   \n",
       "17521539    ...               UD       0               1             1   \n",
       "\n",
       "         cDNApos.na CDSpos.na protPos.na Grantham.na PolyPhenVal.na SIFTval.na  \n",
       "17521530          1         1          1           1              1          1  \n",
       "17521531          1         1          1           1              1          1  \n",
       "17521532          0         0          0           0              0          0  \n",
       "17521533          1         1          1           1              1          1  \n",
       "17521534          1         1          1           1              1          1  \n",
       "17521535          1         1          1           1              1          1  \n",
       "17521536          1         1          1           1              1          1  \n",
       "17521537          1         1          1           1              1          1  \n",
       "17521538          1         1          1           1              1          1  \n",
       "17521539          1         1          1           1              1          1  \n",
       "\n",
       "[10 rows x 103 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 249 ms, sys: 46.8 ms, total: 295 ms\n",
      "Wall time: 295 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "myframe = next(get_first_n_batches(training, 1, delimiter='\\t'))\n",
    "# myframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 45s, sys: 2min 13s, total: 23min 59s\n",
      "Wall time: 24min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# myotherframe = next(get_batch_from_line_skip(training, 3000000, delimiter='\\t')) # 29.3 s\n",
    "# myotherframe = next(get_batch_from_line_isslice(training, 17000000, delimiter='\\t')) # 4min 5s\n",
    "# myotherframe = next(get_batch_from_line_isslice(training_imputed, 17000000)) # 23min 59s\n",
    "# myotherframe = next(get_batch_from_line(3000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>RefxA</th>\n",
       "      <th>RefxC</th>\n",
       "      <th>RefxG</th>\n",
       "      <th>RefxT</th>\n",
       "      <th>RefxN</th>\n",
       "      <th>AltxA</th>\n",
       "      <th>AltxC</th>\n",
       "      <th>AltxG</th>\n",
       "      <th>AltxT</th>\n",
       "      <th>...</th>\n",
       "      <th>YxM</th>\n",
       "      <th>YxN</th>\n",
       "      <th>YxP</th>\n",
       "      <th>YxQ</th>\n",
       "      <th>YxR</th>\n",
       "      <th>YxS</th>\n",
       "      <th>YxT</th>\n",
       "      <th>YxV</th>\n",
       "      <th>YxW</th>\n",
       "      <th>YxY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17999995</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17999996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17999997</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17999998</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17999999</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1064 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          y RefxA RefxC RefxG RefxT RefxN AltxA AltxC AltxG AltxT ... YxM YxN  \\\n",
       "17999995  1     0     0     1     0     0     0     0     0     1 ...   0   0   \n",
       "17999996  1     0     0     1     0     0     0     1     0     0 ...   0   0   \n",
       "17999997  1     0     1     0     0     0     1     0     0     0 ...   0   0   \n",
       "17999998  1     0     0     1     0     0     1     0     0     0 ...   0   0   \n",
       "17999999  1     0     0     1     0     0     0     1     0     0 ...   0   0   \n",
       "\n",
       "         YxP YxQ YxR YxS YxT YxV YxW YxY  \n",
       "17999995   0   0   0   0   0   0   0   0  \n",
       "17999996   0   0   0   0   0   0   0   0  \n",
       "17999997   0   0   0   0   0   0   0   0  \n",
       "17999998   0   0   0   0   0   0   0   0  \n",
       "17999999   0   0   0   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 1064 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.unique(myframe.y)\n",
    "#np.unique(myotherframe.y)\n",
    "#myotherframe.shape\n",
    "myotherframe.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/modules/i12g/anaconda/3-5.0.1/envs/kipoi-cadd2/lib/python3.6/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "y = training_df.y\n",
    "X = training_imp_df.drop(columns=\"y\")\n",
    "\n",
    "# Inserting some artificial positive examples\n",
    "np.random.seed(10)\n",
    "msk = np.random.rand(len(y)) < 0.2\n",
    "y[msk] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droped 824 constant cols.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/modules/i12g/anaconda/3-5.0.1/envs/kipoi-cadd2/lib/python3.6/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# Drop constant columns\n",
    "constant_cols = set()\n",
    "dataset = X\n",
    "for col in dataset:\n",
    "    # print(dataset.shape, type(dataset))\n",
    "    # print(len(np.unique(dataset[col])))\n",
    "    if len(np.unique(dataset[col])) < 20 and col != 'y':\n",
    "        constant_cols.add(col)\n",
    "print(\"Droped\", len(constant_cols), \"constant cols.\")\n",
    "\n",
    "for dataset in [X_train, X_test, y_train, y_test]:\n",
    "    dataset.drop(columns=list(constant_cols), inplace=True, errors='ignore')\n",
    "\n",
    "X.drop(columns=list(constant_cols), inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=10,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', solver='lbfgs', n_jobs=10)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read like \n",
      " tn, fp, \n",
      " fn, tp \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6304,    0],\n",
       "       [1695,    1]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Read like \\n tn, fp, \\n fn, tp \\n\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, y_train_pred).ravel()\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kipoi-cadd2]",
   "language": "python",
   "name": "conda-env-kipoi-cadd2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
