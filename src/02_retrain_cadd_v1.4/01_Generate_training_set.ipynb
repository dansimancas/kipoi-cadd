{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from kipoi_cadd.utils import dump_to_pickle, load_pickle, get_all_files_extension, generate_variant_ids\n",
    "from kipoi_cadd.data_utils import load_csv_chunks_tosparse\n",
    "from kipoi_cadd.data import sparse_cadd_dataset\n",
    "from scipy.sparse import vstack, load_npz, save_npz\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = \"/s/project/kipoi-cadd/data/raw/v1.4/training_data/\"\n",
    "training_dir_hg37 = os.path.join(training_dir, \"GRCh37\")\n",
    "training_dir_hg38 = os.path.join(training_dir, \"GRCh38\")\n",
    "variant_ids_dir_hg37 = os.path.join(training_dir_hg37, \"variant_ids\")\n",
    "variant_ids_dir_hg38 = os.path.join(training_dir_hg38, \"variant_ids\")\n",
    "sparse_matrices_dir_hg37 = os.path.join(training_dir_hg37, \"sparse_matrices\")\n",
    "sparse_matrices_dir_hg38 = os.path.join(training_dir_hg38, \"sparse_matrices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training set\n",
    "This includes converting all csv files to sparse and keeping the variant ids\n",
    "### 1. Generate variant ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_variant_ids(inputfile, outputfile, separator='\\t', variant_cols=['Chrom', 'Pos', 'Ref', 'Alt']):\n",
    "    print(inputfile)\n",
    "    input_df = pd.read_csv(inputfile,\n",
    "                           sep=separator,\n",
    "                           usecols=variant_cols,\n",
    "                           nrows=1000,\n",
    "                           dtype={\n",
    "                               'Chrom': 'str',\n",
    "                               'Pos': np.int32,\n",
    "                               'Ref': 'str',\n",
    "                               'Alt': 'str'})\n",
    "    \n",
    "    variant_ids = input_df.apply(\n",
    "        lambda row: ':'.join([str(row[0]), str(row[1]), row[2],\n",
    "                              str(row[3].split(','))]), axis=1)\n",
    "    \n",
    "    print(outputfile)\n",
    "    dump_to_pickle(outputfile, variant_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a GRCh37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ext = \"tsv.gz\"\n",
    "for file in get_all_files_extension(training_dir_hg37, ext):\n",
    "    # if \"InDels\" in file: continue\n",
    "    out = os.path.join(training_dir_hg37, \"variant_ids\", os.path.basename(file).split(\".\" + ext)[0] + \".pkl\")\n",
    "    generate_variant_ids(file, out, variant_cols=['#Chr', 'Pos', 'Ref', 'Alt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b GRCh38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ext = \"tsv.gz\"\n",
    "for file in get_all_files_extension(training_dir_hg38, ext):\n",
    "    out = os.path.join(training_dir_hg38, \"variant_ids\", os.path.basename(file).split(\".\" + ext)[0] + \".pkl\")\n",
    "    generate_variant_ids(file, out, variant_cols=['#Chrom', 'Pos', 'Ref', 'Alt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sparse matrices\n",
    "For v1.4 the training set comes in shape of separated files. As explained in the INFO file: _## *.csv.gz Imputed and transformed training set (directly usable as X in any machine learning setting with first column being Y)_. Namely:\n",
    "- `humanDerived_InDels.csv.gz`\n",
    "- `humanDerived_SNVs.csv.gz`\n",
    "- `simulation_InDels.csv.gz`\n",
    "- `simulation_SNVs.csv.gz``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a GRCh37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = \"csv.gz\"\n",
    "for f in get_all_files_extension(training_dir_hg37, ext):\n",
    "    # Get the base of the name\n",
    "    f_name = os.path.basename(f).split(\".\" + ext)[0]\n",
    "    if f_name == \"humanDerived_InDels\": continue\n",
    "    # Num lines is necessary to set the total in tqdm, important feedback in a lengthy function\n",
    "    num_lines = len(load_pickle(os.path.join(training_dir_hg37, \"variant_ids\", f_name + \".pkl\")))\n",
    "    output = os.path.join(training_dir_hg37, \"sparse_matrices\", f_name + \".npz\")\n",
    "    print(f_name, num_lines, output)\n",
    "    load_csv_chunks_tosparse(f, 10000, np.float32, num_lines=num_lines, output=output, header=None)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b GRCh38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = \"csv.gz\"\n",
    "for f in get_all_files_extension(training_dir_hg38, ext):\n",
    "    # Get the base of the name\n",
    "    f_name = os.path.basename(f).split(\".\" + ext)[0]\n",
    "    if f_name == \"humanDerived_InDels\": continue\n",
    "    # Num lines is necessary to set the total in tqdm, important feedback in a lengthy function\n",
    "    num_lines = len(load_pickle(os.path.join(training_dir_hg38, \"variant_ids\", f_name + \".pkl\")))\n",
    "    output = os.path.join(training_dir_hg38, \"sparse_matrices\", f_name + \".npz\")\n",
    "    print(f_name, num_lines, output)\n",
    "    load_csv_chunks_tosparse(f, 10000, np.float32, num_lines=num_lines, output=output, header=None)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge sparse matrices\n",
    "### 3 Merge variant ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = \"pkl\"\n",
    "all_ids = None\n",
    "for f in tqdm(get_all_files_extension(variant_ids_dir_hg37, ext)):\n",
    "    if all_ids is None:\n",
    "        all_ids = load_pickle(f)\n",
    "    else:\n",
    "        all_ids = pd.concat([all_ids, load_pickle(f)], ignore_index=True)\n",
    "\n",
    "print(len(all_ids))\n",
    "output = os.path.join(variant_ids_dir_hg37, \"all.pkl\")\n",
    "dump_to_pickle(output, all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = \"pkl\"\n",
    "all_ids = None\n",
    "for f in tqdm(get_all_files_extension(variant_ids_dir_hg38, ext)):\n",
    "    if all_ids is None:\n",
    "        all_ids = load_pickle(f)\n",
    "    else:\n",
    "        all_ids = pd.concat([all_ids, load_pickle(f)], ignore_index=True)\n",
    "\n",
    "print(len(all_ids))\n",
    "output = os.path.join(variant_ids_dir_hg38, \"all.pkl\")\n",
    "dump_to_pickle(output, all_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a GRCh37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = \"npz\"\n",
    "all_npz = None\n",
    "for f in tqdm(get_all_files_extension(sparse_matrices_dir_hg37, ext)):\n",
    "    if all_npz is None:\n",
    "        all_npz = load_npz(f)\n",
    "    else:\n",
    "        all_npz = vstack([all_npz, load_npz(f)])\n",
    "output = os.path.join(sparse_matrices_dir_hg37, \"all.npz\")\n",
    "save_npz(output, all_npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that y is binary\n",
    "all_npz = load_npz(os.path.join(sparse_matrices_dir_hg37, \"all.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_npz[:,0].nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b GRCh38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = \"npz\"\n",
    "all_npz = None\n",
    "for f in tqdm(get_all_files_extension(sparse_matrices_dir_hg38, ext)):\n",
    "    if all_npz is None:\n",
    "        all_npz = load_npz(f)\n",
    "    else:\n",
    "        all_npz = vstack([all_npz, load_npz(f)])\n",
    "output = os.path.join(sparse_matrices_dir_hg38, \"all.npz\")\n",
    "save_npz(output, all_npz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and validation set\n",
    "### 4.a GRCh37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_ids_file = os.path.join(variant_ids_dir_hg37, \"all.pkl\")\n",
    "s = os.path.join(sparse_matrices_dir_hg37, \"all.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, train_ids), (valid, valid_ids) = sparse_cadd_dataset(s, variant_ids_file, output_npz=sparse_matrices_dir_hg37, output_ids=variant_ids_dir_hg37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1286248, 905) 1286248 (551250, 905) 551250\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, len(train_ids), valid.shape, len(valid_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b GRCh38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_ids_file = os.path.join(variant_ids_dir_hg38, \"all.pkl\")\n",
    "s = os.path.join(sparse_matrices_dir_hg38, \"all.npz\")\n",
    "(train, train_ids), (valid, valid_ids) = sparse_cadd_dataset(s, variant_ids_file, output_npz=sparse_matrices_dir_hg38, output_ids=variant_ids_dir_hg38)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kipoi-cadd2]",
   "language": "python",
   "name": "conda-env-kipoi-cadd2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
